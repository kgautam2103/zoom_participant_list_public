{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "zoom_participant_list_public.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBId67ZDntjidwdJbeI6wP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgautam2103/zoom_participant_list_public/blob/main/zoom_participant_list_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgQ3vGvltzT9"
      },
      "source": [
        "# when you run this code, you will get a link below\n",
        "#Follow the link, copy the code, paste it in the box and then press Enter on the keyboard.\n",
        "#This will authenticate you and will allow you to interact with Google Sheets and other Google apps in your notebook.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Crh7QbMYyCbQ"
      },
      "source": [
        "import requests\n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime,timedelta\n",
        "import gspread\n",
        "import re\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "from pandas import json_normalize\n",
        "from urllib.parse import urlencode\n",
        "!pip install python-Levenshtein\n",
        "!pip install fuzzywuzzy\n",
        "from fuzzywuzzy import process\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5TRT41tZtwk"
      },
      "source": [
        "report_date= input(\"enter the report date like 2021-07-11 : \")\n",
        "move2side_input=input('move2side needed ? Enter true or false: ').lower()\n",
        "#report_date = '2021-07-18'\n",
        "apac_report_datetime = str(report_date)+'T01'\n",
        "naol_report_datetime = str(report_date)+'T13'\n",
        "meetingID = '9181716151'\n",
        "base_url = 'https://api.zoom.us/v2/'\n",
        "past_meetings = base_url+'/past_meetings/'\n",
        "\n",
        "\n",
        "#to get result for the last held meeting\n",
        "report_meetings = base_url+'report/meetings/'\n",
        "#get_meeting_url = report_meetings+meetingID\n",
        "\n",
        "get_meeting_url = past_meetings+meetingID+'/instances'\n",
        "get_user_url = base_url+'users/'\n",
        "apac_uid=''\n",
        "naol_uid=''\n",
        "page_size ='300'\n",
        "apac_participants_list = []\n",
        "naol_participants_list = []\n",
        "apac_next_page_token = ''\n",
        "naol_next_page_token = ''\n",
        "apac_participants_df=pd.DataFrame()\n",
        "naol_participants_df=pd.DataFrame()\n",
        "\n",
        "\n",
        "current_ts = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "output_sheet = current_ts+'_participants_list_'+report_date\n",
        "sh = gc.create(output_sheet)\n",
        "\n",
        "final_output_sheet = sh.add_worksheet('final_combined',rows=\"5000\", cols=\"10\")\n",
        "apac_output_sheet = sh.add_worksheet('APAC',rows=\"500\", cols=\"10\")\n",
        "naol_output_sheet = sh.add_worksheet('NAOL',rows=\"500\", cols=\"10\")\n",
        "rolling3week_output_sheet = sh.add_worksheet('3weekRollingWeekend',rows=\"5000\", cols=\"10\")\n",
        "move2side_output_sheet = sh.add_worksheet('move2side',rows=\"500\", cols=\"10\")\n",
        "updated_regular_output_sheet = sh.add_worksheet('updated_regular_sideroom',rows=\"1000\", cols=\"10\")\n",
        "apac_poll1_sheet = sh.add_worksheet('APAC_poll1',rows=\"500\", cols=\"10\")\n",
        "apac_poll2_sheet = sh.add_worksheet('APAC_poll2',rows=\"500\", cols=\"10\")\n",
        "naol_poll1_sheet = sh.add_worksheet('NAOL_poll1',rows=\"500\", cols=\"10\")\n",
        "naol_poll2_sheet = sh.add_worksheet('NAOL_poll2',rows=\"500\", cols=\"10\")\n",
        "\n",
        "dummy_sheet = sh.get_worksheet(0)\n",
        "sh.del_worksheet(dummy_sheet)\n",
        "\n",
        "wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1WhACIU6F7i9VwDxXoqzx6GEwutQI5HpIYl4v6H49-7Q/edit#gid=0')\n",
        "\n",
        "input_sheet = wb.worksheet('sahajyogis')\n",
        "inputdata = input_sheet.get_all_values()\n",
        "inputdata_df = pd.DataFrame(inputdata, columns=['id','name','email'])\n",
        "inputdata_df = inputdata_df.drop(inputdata_df.index[0])\n",
        "\n",
        "rolling_3week_sheet = wb.worksheet('Rolling3Weekend')\n",
        "rolling_3week_data = rolling_3week_sheet.get_all_values()\n",
        "rolling_3week_df = pd.DataFrame(rolling_3week_data, columns=['event','event_date','id','name', 'user_email', 'join_time', 'leave_time', 'duration'])\n",
        "if rolling_3week_df.shape[0] > 0:\n",
        "  rolling_3week_df = rolling_3week_df.drop(rolling_3week_df.index[0])\n",
        "\n",
        "weekend_archive_sheet = wb.worksheet('WeekendArchive')\n",
        "weekend_archive_data = weekend_archive_sheet.get_all_values()\n",
        "weekend_archive_df = pd.DataFrame(weekend_archive_data, columns=['event','event_date','id','name', 'user_email', 'join_time', 'leave_time', 'duration'])\n",
        "if weekend_archive_df.shape[0] > 0:\n",
        "  weekend_archive_df = weekend_archive_df.drop(weekend_archive_df.index[0])\n",
        "\n",
        "Sideroom_sheet = wb.worksheet('regular_sideroom')\n",
        "sideroom_data = Sideroom_sheet.get_all_values()\n",
        "sideroom_data_df = pd.DataFrame(sideroom_data, columns=['name'])\n",
        "sideroom_data_df = sideroom_data_df.drop(sideroom_data_df.index[0])\n",
        "\n",
        "move2side_sheet = wb.worksheet('move2side')\n",
        "updated_sideroom_sheet = wb.worksheet('updated_sideroom')\n",
        "month_3_archive_frequency_sheet = wb.worksheet('3month_archive_frequency')\n",
        "\n",
        "report_datetime = datetime.strptime(report_date, '%Y-%m-%d')\n",
        "report_datetime_3week_before = report_datetime-timedelta(days=15)\n",
        "report_datetime_3month_before = report_datetime-timedelta(days=90)\n",
        "\n",
        "ref_id_list = list(filter(None, inputdata_df['id'].tolist()))\n",
        "ref_name_list = list(filter(None, inputdata_df['name'].tolist()))\n",
        "ref_name_list = [i.lower() for i in ref_name_list]\n",
        "ref_email_list = list(filter(None, inputdata_df['email'].tolist()))\n",
        "\n",
        "\n",
        "#enter the zoom account auth token\n",
        "auth_token = ''\n",
        "\n",
        "\n",
        "\n",
        "authorization = 'Bearer ' + auth_token\n",
        "auth_headers = {\n",
        "    'Authorization': authorization,\n",
        "    'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "meeting_id_response = requests.get(get_meeting_url,headers=auth_headers)\n",
        "\n",
        "\n",
        "if meeting_id_response.status_code == 200:\n",
        "\n",
        "  for x in meeting_id_response.json()['meetings']:\n",
        "    if apac_report_datetime in str(x['start_time']):\n",
        "      apac_uid = str(x['uuid'])\n",
        "      print(x)\n",
        "    if naol_report_datetime in str(x['start_time']):\n",
        "      naol_uid = str(x['uuid'])\n",
        "      print(x)\n",
        "\n",
        "\n",
        "\n",
        "print('apac_id : '+apac_uid)\n",
        "print('naol_id : '+naol_uid)\n",
        "\n",
        "if apac_uid != \"\":\n",
        "  encoded_apac_uid = urlencode({'id':apac_uid})\n",
        "  double_encoded_apac_id = urlencode({'id':encoded_apac_uid[3:]})\n",
        "  apac_uid = double_encoded_apac_id[3:]\n",
        "  print('apac_id_encoded : '+apac_uid)\n",
        "  apac_poll_response_url = base_url+'report/meetings/'+apac_uid+'/polls'\n",
        "  get_apac_participants_url = report_meetings+apac_uid+'/participants?page_size='+page_size\n",
        "  apac_participants_response = requests.get(get_apac_participants_url,headers=auth_headers)\n",
        "  #print(apac_participants_response.json())\n",
        "  apac_participants_list = apac_participants_response.json()['participants']\n",
        "  apac_next_page_token = apac_participants_response.json()['next_page_token']\n",
        "  while apac_next_page_token != \"\":\n",
        "    new_get_apac_participants_url = get_apac_participants_url+'&next_page_token='+apac_next_page_token\n",
        "    new_apac_participants_response = requests.get(new_get_apac_participants_url,headers=auth_headers)\n",
        "    apac_next_page_token =  new_apac_participants_response.json()['next_page_token']\n",
        "    apac_participants_list.extend(new_apac_participants_response.json()['participants'])\n",
        "\n",
        "  apac_poll_response = requests.get(apac_poll_response_url,headers = auth_headers)\n",
        "  apac_response_list = apac_poll_response.json()['questions']\n",
        "  apac_poll_answer_df = json_normalize(apac_response_list,\n",
        "                                record_path=['question_details'],\n",
        "                                meta=['name', 'email',],\n",
        "                                errors='ignore')\n",
        "\n",
        "  apac_poll1_df = apac_poll_answer_df[apac_poll_answer_df.polling_id=='l1gX0McLRc6mxffk37nnIw']\n",
        "  if apac_poll1_df.shape[0] > 0:\n",
        "    apac_poll1_df  = apac_poll1_df.fillna(\"\")\n",
        "    apac_poll1_df = apac_poll1_df.reset_index()\n",
        "    apac_poll1_df = apac_poll1_df.drop(columns=['question','polling_id','date_time','email','index'])\n",
        "    apac_poll1_df = apac_poll1_df[['name','answer']]\n",
        "    set_with_dataframe(apac_poll1_sheet, apac_poll1_df)\n",
        "\n",
        "  apac_poll2_df = apac_poll_answer_df[apac_poll_answer_df.polling_id=='mlE7n7f3TQ-qgXoF--MKPQ']\n",
        "  if apac_poll2_df.shape[0] > 0:\n",
        "    apac_poll2_df  = apac_poll2_df.fillna(\"\")\n",
        "    apac_poll2_df['question'] = apac_poll2_df['question'].apply(lambda x: re.sub('^[\\.\\d\\s]*','',x,flags=re.IGNORECASE))\n",
        "    apac_poll2_df.drop_duplicates(subset=['name','question'],inplace=True)\n",
        "    apac_poll2_df = apac_poll2_df.pivot(index=\"name\", columns=\"question\", values=\"answer\")\n",
        "    apac_poll2_df.reset_index(inplace=True)\n",
        "    apac_poll2_df[['answer1','answer2','answer3']] = apac_poll2_df[['Will you be joining us next weekend?',\n",
        "                                          'Which of the following mid-week followup sessions are convenenient for you? We look forward to see you',\n",
        "                                          'How was your thoughtless state during meditation?']]\n",
        "    apac_poll2_df=apac_poll2_df.drop(columns=['How was your thoughtless state during meditation?',\n",
        "                                'Will you be joining us next weekend?',\n",
        "                                      'Which of the following mid-week followup sessions are convenenient for you? We look forward to see you'])\n",
        "    set_with_dataframe(apac_poll2_sheet, apac_poll2_df) \n",
        "\n",
        "\n",
        "\n",
        "if naol_uid != \"\":\n",
        "  encoded_naol_uid = urlencode({'id':naol_uid})\n",
        "  double_encoded_naol_id = urlencode({'id':encoded_naol_uid[3:]})\n",
        "  naol_uid = double_encoded_naol_id[3:]\n",
        "  print('naol_id_encoded : '+naol_uid)\n",
        "  naol_poll_response_url = base_url+'report/meetings/'+naol_uid+'/polls'\n",
        "  get_naol_participants_url = report_meetings+naol_uid+'/participants?page_size='+page_size\n",
        "  naol_participants_response = requests.get(get_naol_participants_url,headers=auth_headers)\n",
        "  #print(naol_participants_response.json()['next_page_token'])\n",
        "  naol_participants_list = naol_participants_response.json()['participants']\n",
        "  naol_next_page_token = naol_participants_response.json()['next_page_token']\n",
        "  while naol_next_page_token != \"\":\n",
        "    new_get_naol_participants_url = get_naol_participants_url+'&next_page_token='+naol_next_page_token\n",
        "    new_naol_participants_response = requests.get(new_get_naol_participants_url,headers=auth_headers)\n",
        "    naol_next_page_token =  new_naol_participants_response.json()['next_page_token']\n",
        "    naol_participants_list.extend(new_naol_participants_response.json()['participants'])\n",
        "\n",
        "  naol_poll_response = requests.get(naol_poll_response_url,headers = auth_headers)\n",
        "  naol_response_list = naol_poll_response.json()['questions']\n",
        "  naol_poll_answer_df = json_normalize(naol_response_list,\n",
        "                                record_path=['question_details'],\n",
        "                                meta=['name', 'email',],\n",
        "                                errors='ignore')\n",
        "\n",
        "  naol_poll1_df = naol_poll_answer_df[naol_poll_answer_df.polling_id=='l1gX0McLRc6mxffk37nnIw']\n",
        "  if naol_poll1_df.shape[0] > 0:\n",
        "    naol_poll1_df  = naol_poll1_df.fillna(\"\")\n",
        "    naol_poll1_df = naol_poll1_df.reset_index()\n",
        "    naol_poll1_df = naol_poll1_df.drop(columns=['question','polling_id','date_time','email','index'])\n",
        "    naol_poll1_df = naol_poll1_df[['name','answer']]\n",
        "    set_with_dataframe(naol_poll1_sheet, naol_poll1_df)\n",
        "\n",
        "  naol_poll2_df = naol_poll_answer_df[naol_poll_answer_df.polling_id=='mlE7n7f3TQ-qgXoF--MKPQ']\n",
        "  if naol_poll2_df.shape[0] > 0:\n",
        "    naol_poll2_df  = naol_poll2_df.fillna(\"\")\n",
        "    naol_poll2_df['question'] = naol_poll2_df['question'].apply(lambda x: re.sub('^[\\.\\d\\s]*','',x,flags=re.IGNORECASE))\n",
        "    naol_poll2_df.drop_duplicates(subset=['name','question'],inplace=True)\n",
        "    naol_poll2_df = naol_poll2_df.pivot(index=\"name\", columns=\"question\", values=\"answer\")\n",
        "    naol_poll2_df.reset_index(inplace=True)\n",
        "    naol_poll2_df[['answer1','answer2','answer3']] = naol_poll2_df[['Will you be joining us next weekend?',\n",
        "                                          'Which of the following mid-week followup sessions are convenenient for you? We look forward to see you',\n",
        "                                          'How was your thoughtless state during meditation?']]\n",
        "    naol_poll2_df=naol_poll2_df.drop(columns=['How was your thoughtless state during meditation?',\n",
        "                                'Will you be joining us next weekend?',\n",
        "                                      'Which of the following mid-week followup sessions are convenenient for you? We look forward to see you'])\n",
        "    set_with_dataframe(naol_poll2_sheet, naol_poll2_df) \n",
        "\n",
        "print(len(apac_participants_list))\n",
        "print(len(naol_participants_list))\n",
        "\n",
        "\n",
        "if len(apac_participants_list)>0:\n",
        "  apac_participants_df = pd.DataFrame(apac_participants_list)\n",
        "  apac_participants_df = apac_participants_df.drop(columns=['attentiveness_score','failover','customer_key'])\n",
        "  apac_participants_df = apac_participants_df[apac_participants_df.duration>600]\n",
        "  apac_participants_df['id'] = apac_participants_df['id'].apply(lambda x: str(random.randint(1,99999999999)) if x==\"\" else x)\n",
        "  apac_participants_df = apac_participants_df.drop_duplicates(subset=['id'])\n",
        "  apac_participants_df['name'] = apac_participants_df['name'].apply(lambda x: re.sub('^sy[\\.\\s_-]*','',x,flags=re.IGNORECASE))\\\n",
        "  .apply(lambda x: re.sub('[\\.\\s_-]*sy$','',x,flags=re.IGNORECASE))\n",
        "  apac_participants_df = apac_participants_df.drop_duplicates(subset=['name'])\n",
        "  apac_participants_df = apac_participants_df[~apac_participants_df.id.isin(ref_id_list)]\n",
        "  apac_participants_df = apac_participants_df[~(apac_participants_df.name).str.lower().isin(ref_name_list)]\n",
        "  apac_participants_df = apac_participants_df[~apac_participants_df.user_email.isin(ref_email_list)]\n",
        "  apac_participants_df['event'] = 'APAC'\n",
        "  apac_participants_df['event_date'] = report_datetime\n",
        "  apac_participants_df = apac_participants_df[['event','event_date','id','name','user_email', 'join_time', 'leave_time', 'duration']]\n",
        "  set_with_dataframe(apac_output_sheet, apac_participants_df)\n",
        "  \n",
        "\n",
        "if len(naol_participants_list)>0:\n",
        "  naol_participants_df = pd.DataFrame(naol_participants_list)\n",
        "  naol_participants_df = naol_participants_df.drop(columns=['attentiveness_score','failover','customer_key'])\n",
        "  naol_participants_df = naol_participants_df[naol_participants_df.duration>600]\n",
        "  naol_participants_df['id'] = naol_participants_df['id'].apply(lambda x: str(random.randint(1,99999999999)) if x==\"\" else x)\n",
        "  naol_participants_df = naol_participants_df.drop_duplicates(subset=['id'])\n",
        "  naol_participants_df['name'] = naol_participants_df['name'].apply(lambda x: re.sub('^sy[\\.\\s_-]+','',x,flags=re.IGNORECASE))\\\n",
        "  .apply(lambda x: re.sub('[\\.\\s_-]+sy$','',x,flags=re.IGNORECASE))\n",
        "  naol_participants_df = naol_participants_df.drop_duplicates(subset=['name'])\n",
        "  naol_participants_df = naol_participants_df[~naol_participants_df.id.isin(ref_id_list)]\n",
        "  naol_participants_df = naol_participants_df[~(naol_participants_df.name).str.lower().isin(ref_name_list)]\n",
        "  naol_participants_df = naol_participants_df[~naol_participants_df.user_email.isin(ref_email_list)]\n",
        "  naol_participants_df['event'] = 'NAOL'\n",
        "  naol_participants_df['event_date'] = report_datetime\n",
        "  naol_participants_df = naol_participants_df[['event','event_date','id','name','user_email', 'join_time', 'leave_time', 'duration']]\n",
        "  set_with_dataframe(naol_output_sheet, naol_participants_df)\n",
        "\n",
        "\n",
        "#columns=['event','event_date','id','name', 'user_email', 'join_time', 'leave_time', 'duration']\n",
        "final_combined_df = apac_participants_df.append(naol_participants_df)\n",
        "set_with_dataframe(final_output_sheet, final_combined_df)\n",
        "\n",
        "if move2side_input == 'true':\n",
        "    \n",
        "  rolling_3week_df_combined = rolling_3week_df.append(final_combined_df)\n",
        "  weekend_archive_df_final = weekend_archive_df.append(final_combined_df)\n",
        "  rolling_3week_df_combined['event_date'] = pd.to_datetime(rolling_3week_df_combined['event_date'])\n",
        "  rolling_3week_df_final = rolling_3week_df_combined[rolling_3week_df_combined.event_date > report_datetime_3week_before]\n",
        "  rolling_3week_df_final['name']=rolling_3week_df_final['name'].str.lower()\n",
        "  rolling_3week_df_final.drop_duplicates(subset=['event','event_date','name'],inplace=True)\n",
        "  weekend_archive_df_final['event_date'] = pd.to_datetime(weekend_archive_df_final['event_date'])\n",
        "  weekend_archive_df_final['name']=weekend_archive_df_final['name'].str.lower()\n",
        "  \n",
        "  weekend_archive_sheet.clear()\n",
        "  rolling_3week_sheet.clear()\n",
        "  set_with_dataframe(rolling3week_output_sheet, rolling_3week_df_final)\n",
        "  set_with_dataframe(weekend_archive_sheet,weekend_archive_df_final)\n",
        "  set_with_dataframe(rolling_3week_sheet, rolling_3week_df_final)\n",
        "\n",
        "  weekend_archive_df_final_filtered_3m = weekend_archive_df_final[weekend_archive_df_final.event_date > report_datetime_3month_before]\n",
        "  weekend_archive_df_final_filtered_3m_frequency = weekend_archive_df_final_filtered_3m.groupby(['name']).agg(count=('name',np.count_nonzero))\n",
        "  weekend_archive_df_final_filtered_3m_frequency.reset_index(inplace=True)\n",
        "  weekend_archive_df_final_filtered_3m_frequency.sort_values(by = ['count'],ascending=False,inplace=True)\n",
        "  set_with_dataframe(month_3_archive_frequency_sheet,weekend_archive_df_final_filtered_3m_frequency)\n",
        "\n",
        "  weekend_archive_group_df = weekend_archive_df_final.groupby('name')\n",
        "  archive_max_date = weekend_archive_group_df.agg(event_date=('event_date',np.max))\n",
        "  archive_max_date.reset_index(inplace=True)\n",
        "  archive_max_date_final = archive_max_date[archive_max_date.event_date > report_datetime_3month_before]\n",
        "\n",
        "  archive_group_df_max_date_details = pd.merge(weekend_archive_df_final,archive_max_date_final,how='inner',on=['name','event_date'])\n",
        "  archive_group_df_max_date_details.drop_duplicates(subset=['name','event_date'],inplace=True)\n",
        "  archive_group_df_max_date_details.drop(columns=['join_time','leave_time','duration'],inplace=True)\n",
        "  #archive_group_df_max_date_details\n",
        "\n",
        "  archive_name_list=archive_group_df_max_date_details['name'].tolist()\n",
        "  archive_name_list_new = [i for i in archive_name_list if len(i) > 1]\n",
        "  matched_name_list=[]\n",
        "  for index,row in sideroom_data_df.iterrows():\n",
        "    highest_match = process.extractOne(row['name'],archive_name_list_new)\n",
        "    matched_name_list.append([row['name'],highest_match[0],highest_match[1]])\n",
        "\n",
        "  matched_name_df = pd.DataFrame(matched_name_list,columns=['name','matched_names','score'])\n",
        "  matched_name_df = matched_name_df[matched_name_df.score>90]\n",
        "  matched_name_df_final = pd.DataFrame(matched_name_df['matched_names'])\n",
        "  matched_name_df_final.rename(columns={'matched_names':'name'},inplace=True)\n",
        "  matched_name_df_final_details = pd.merge(archive_group_df_max_date_details,matched_name_df_final,how='inner',on='name')\n",
        "\n",
        "  matched_name_df_final_details['event_date'] = matched_name_df_final_details['event_date'].dt.strftime('%Y-%m-%d')\n",
        "  matched_name_df_final_details['last_event']=matched_name_df_final_details['event_date'].str.slice(0,10)+\" \"+matched_name_df_final_details['event']\n",
        "  matched_name_df_final_details.drop(columns=['event','event_date','id'],inplace=True)\n",
        "\n",
        "  updated_sideroom_sheet.clear()\n",
        "  set_with_dataframe(updated_sideroom_sheet, matched_name_df_final_details)\n",
        "  set_with_dataframe(updated_regular_output_sheet, matched_name_df_final_details)\n",
        "\n",
        "\n",
        "  group_name_df = rolling_3week_df_final.groupby(['name'])\n",
        "  count_name = group_name_df.agg(count=('name',np.count_nonzero))\n",
        "  max_date = group_name_df.agg(latest_date=('event_date',np.max) )\n",
        "  max_date.reset_index(inplace=True)\n",
        "  count_name.reset_index(inplace=True)\n",
        "  final_max_count = pd.merge(count_name,max_date,on='name')\n",
        "  final_max_count_filtered = final_max_count[final_max_count['count']>1]\n",
        "\n",
        "  regular_matched_name_list=matched_name_df_final_details['name'].tolist()\n",
        "  regular_matched_name_list_new = [i for i in regular_matched_name_list if len(i) > 1]\n",
        "  move2side_name_list=[]\n",
        "  for index,row in final_max_count_filtered.iterrows():\n",
        "    highest_match_move2side = process.extractOne(row['name'],regular_matched_name_list_new)\n",
        "    move2side_name_list.append([row['name'],highest_match_move2side[0],highest_match_move2side[1]])\n",
        "\n",
        "  move2side_name_df = pd.DataFrame(move2side_name_list,columns=['name','matched_names','score'])\n",
        "  move2side_name_df = move2side_name_df[move2side_name_df.score>90]\n",
        "  move2side_name_df_final = pd.DataFrame(move2side_name_df['matched_names'])\n",
        "  move2side_name_df_final_list = move2side_name_df_final['matched_names'].tolist()\n",
        "  final_max_count_filtered = final_max_count_filtered[~final_max_count_filtered.name.isin(move2side_name_df_final_list)]\n",
        "  final_max_count_filtered.drop(columns=['count'],inplace=True)\n",
        "\n",
        "  email_name_ref_df = weekend_archive_df_final[['name','user_email']]\n",
        "  email_name_ref_df.drop_duplicates()\n",
        "  nan_value = float(\"NaN\")\n",
        "  email_name_ref_df.replace(\"\", nan_value, inplace=True)\n",
        "  email_name_ref_df.dropna(inplace=True)\n",
        "\n",
        "  move2side_final_df_with_email = pd.merge(final_max_count_filtered,email_name_ref_df,how='left',on='name')\n",
        "  move2side_final_df_with_email.drop_duplicates(inplace=True)\n",
        "\n",
        "  move2side_sheet.clear()\n",
        "  set_with_dataframe(move2side_sheet, move2side_final_df_with_email)\n",
        "  set_with_dataframe(move2side_output_sheet,move2side_final_df_with_email)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}